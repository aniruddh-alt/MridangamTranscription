
# Mridangam Transcription Model on Kaggle
# This notebook runs the mridangam transcription model

import sys
import os
from pathlib import Path

# Add code directory to path
sys.path.append('/kaggle/input/mridangam-transcription/code')

# Import functions from our modules
from data_preparation import get_audio, get_mel_spectrogram, get_window, get_onset
from dataset_creation_kaggle import MridangamDataset, compute_mel_statistics, create_file_based_dataset, create_efficient_dataloader

# Set up paths for Kaggle
data_path = Path('/kaggle/input/mridangam-transcription/data/mridangam_stroke_1.0')

# Import and set up your model
import torch
import torch.nn as nn
import torch.optim as optim
from CNN import CNNModel  # Import your model architecture

# Hyperparameters
batch_size = 32
learning_rate = 0.001
num_epochs = 50
architecture = 'cnn'

# Create datasets and dataloaders
print("Creating datasets...")
data = create_file_based_dataset(
    directory=data_path,
    test_size=0.2,
    target_length=128,
    architecture=architecture,
    compute_stats=True
)

# Create efficient dataloaders (optimized for Kaggle)
train_loader = create_efficient_dataloader(
    dataset=data['train'],
    batch_size=batch_size,
    shuffle=True,
    num_workers=2  # Kaggle typically has 2-4 CPU cores
)

test_loader = create_efficient_dataloader(
    dataset=data['test'],
    batch_size=batch_size,
    shuffle=False,
    num_workers=2
)

# Set up device (GPU if available)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Initialize model
n_classes = data['num_classes']
model = CNNModel(n_classes=n_classes).to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()  # Using CrossEntropyLoss for multi-class classification
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Define a function to train the model
def train_model(model, train_loader, criterion, optimizer, num_epochs=10):
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            # Zero the parameter gradients
            optimizer.zero_grad()
            
            # Forward + backward + optimize
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            # Statistics
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        train_accuracy = 100.0 * correct / total
        
        # Evaluation
        model.eval()
        test_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                test_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        test_accuracy = 100.0 * correct / total
        
        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%, Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {test_accuracy:.2f}%')
        
        # Save checkpoint every 5 epochs
        if (epoch + 1) % 5 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'test_accuracy': test_accuracy,
            }, f'model_checkpoint_epoch_{epoch+1}.pth')

def evaluate_model(model, test_loader):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    test_accuracy = 100.0 * correct / total
    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')
    return test_accuracy, test_loss/len(test_loader)

# Training and evaluation
train_model(model, train_loader, criterion, optimizer, num_epochs=num_epochs)
evaluate_model(model, test_loader)

# Save final model
torch.save(model.state_dict(), 'mridangam_model_final.pth')
print("Training complete!")
