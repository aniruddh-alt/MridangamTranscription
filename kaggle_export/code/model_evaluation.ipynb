{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01267fca",
   "metadata": {},
   "source": [
    "# Mridangam Model Comprehensive Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation tools for the mridangam transcription model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9549c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, roc_curve, auc\n",
    ")\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9e254",
   "metadata": {},
   "source": [
    "## Load Your Trained Model and Data\n",
    "\n",
    "Make sure you have already trained your model using the main training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model and test data\n",
    "# Assuming you have these from your training script\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model (replace with your model loading code)\n",
    "# model = torch.load('mridangam_model_complete.pth', map_location=device)\n",
    "# test_loader = your_test_loader\n",
    "# label_encoder = your_label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86abd0",
   "metadata": {},
   "source": [
    "## 1. Basic Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(model, test_loader, device):\n",
    "    \"\"\"Get all predictions and true labels from test set\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Getting predictions\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_probabilities)\n",
    "\n",
    "# Get predictions\n",
    "# predictions, true_labels, probabilities = get_predictions_and_labels(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850079ef",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Metrics Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comprehensive_metrics(true_labels, predictions, probabilities, class_names):\n",
    "    \"\"\"Display comprehensive evaluation metrics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Total Test Samples: {len(true_labels)}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 40)\n",
    "    print(classification_report(true_labels, predictions, target_names=class_names))\n",
    "    \n",
    "    # Per-class analysis\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"PER-CLASS ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = true_labels == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_accuracy = accuracy_score(true_labels[class_mask], predictions[class_mask])\n",
    "            class_count = np.sum(class_mask)\n",
    "            predicted_as_this_class = np.sum(predictions == i)\n",
    "            \n",
    "            print(f\"{class_name:>15}: Accuracy={class_accuracy*100:6.2f}% | \"\n",
    "                  f\"True samples={class_count:3d} | Predicted as this={predicted_as_this_class:3d}\")\n",
    "    \n",
    "    # Confidence analysis\n",
    "    max_probs = np.max(probabilities, axis=1)\n",
    "    correct_mask = predictions == true_labels\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Average confidence (all): {np.mean(max_probs)*100:.2f}%\")\n",
    "    print(f\"Average confidence (correct): {np.mean(max_probs[correct_mask])*100:.2f}%\")\n",
    "    print(f\"Average confidence (incorrect): {np.mean(max_probs[~correct_mask])*100:.2f}%\")\n",
    "    \n",
    "    # Low confidence predictions\n",
    "    low_conf_threshold = 0.6\n",
    "    low_conf_mask = max_probs < low_conf_threshold\n",
    "    print(f\"Predictions with confidence < {low_conf_threshold*100:.0f}%: {np.sum(low_conf_mask)} \"\n",
    "          f\"({np.sum(low_conf_mask)/len(true_labels)*100:.1f}%)\")\n",
    "\n",
    "# Display metrics\n",
    "# display_comprehensive_metrics(true_labels, predictions, probabilities, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9bc840",
   "metadata": {},
   "source": [
    "## 3. Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dashboard(true_labels, predictions, probabilities, class_names):\n",
    "    \"\"\"Create comprehensive evaluation dashboard\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    plt.subplot(4, 4, 1)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Normalized Confusion Matrix\n",
    "    plt.subplot(4, 4, 2)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Normalized Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. Per-class metrics\n",
    "    plt.subplot(4, 4, 3)\n",
    "    report = classification_report(true_labels, predictions, target_names=class_names, output_dict=True)\n",
    "    metrics_df = pd.DataFrame(report).T\n",
    "    metrics_df = metrics_df.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
    "    \n",
    "    x_pos = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x_pos - width, metrics_df['precision'], width, label='Precision', alpha=0.8)\n",
    "    plt.bar(x_pos, metrics_df['recall'], width, label='Recall', alpha=0.8)\n",
    "    plt.bar(x_pos + width, metrics_df['f1-score'], width, label='F1-Score', alpha=0.8)\n",
    "    \n",
    "    plt.title('Per-Class Metrics', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x_pos, class_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 4. Class distribution\n",
    "    plt.subplot(4, 4, 4)\n",
    "    unique_labels, label_counts = np.unique(true_labels, return_counts=True)\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "    plt.pie(label_counts, labels=[class_names[i] for i in unique_labels], \n",
    "            autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    plt.title('Test Set Class Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 5. Prediction confidence distribution\n",
    "    plt.subplot(4, 4, 5)\n",
    "    max_probs = np.max(probabilities, axis=1)\n",
    "    correct_mask = predictions == true_labels\n",
    "    \n",
    "    plt.hist(max_probs[correct_mask], bins=30, alpha=0.7, label='Correct', color='green', density=True)\n",
    "    plt.hist(max_probs[~correct_mask], bins=30, alpha=0.7, label='Incorrect', color='red', density=True)\n",
    "    plt.title('Prediction Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Max Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Accuracy vs confidence threshold\n",
    "    plt.subplot(4, 4, 6)\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    accuracies = []\n",
    "    sample_counts = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        high_conf_mask = max_probs >= threshold\n",
    "        if np.sum(high_conf_mask) > 0:\n",
    "            acc = accuracy_score(true_labels[high_conf_mask], predictions[high_conf_mask])\n",
    "            accuracies.append(acc)\n",
    "            sample_counts.append(np.sum(high_conf_mask))\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "            sample_counts.append(0)\n",
    "    \n",
    "    plt.plot(thresholds, accuracies, 'b-o', linewidth=2, markersize=4, label='Accuracy')\n",
    "    plt.title('Accuracy vs Confidence Threshold', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 7. Top misclassifications\n",
    "    plt.subplot(4, 4, 7)\n",
    "    misclass_counts = Counter()\n",
    "    for true_label, pred_label in zip(true_labels, predictions):\n",
    "        if true_label != pred_label:\n",
    "            pair = (class_names[true_label], class_names[pred_label])\n",
    "            misclass_counts[f\"{pair[0]}â†’{pair[1]}\"] += 1\n",
    "    \n",
    "    if misclass_counts:\n",
    "        top_misclass = misclass_counts.most_common(8)\n",
    "        pairs, counts = zip(*top_misclass)\n",
    "        y_pos = np.arange(len(pairs))\n",
    "        plt.barh(y_pos, counts, color='salmon')\n",
    "        plt.title('Top Misclassifications', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Count')\n",
    "        plt.yticks(y_pos, pairs)\n",
    "        plt.gca().invert_yaxis()\n",
    "    \n",
    "    # 8. Prediction confidence by class\n",
    "    plt.subplot(4, 4, 8)\n",
    "    conf_by_class = []\n",
    "    class_labels = []\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = predictions == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_confidences = max_probs[class_mask]\n",
    "            conf_by_class.append(class_confidences)\n",
    "            class_labels.append(class_name)\n",
    "    \n",
    "    if conf_by_class:\n",
    "        plt.boxplot(conf_by_class, labels=class_labels)\n",
    "        plt.title('Prediction Confidence by Class', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Predicted Class')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9-12. Individual class probability distributions\n",
    "    for i in range(4):\n",
    "        if i < len(class_names):\n",
    "            plt.subplot(4, 4, 9+i)\n",
    "            class_probs = probabilities[:, i]\n",
    "            true_class_mask = true_labels == i\n",
    "            other_class_mask = ~true_class_mask\n",
    "            \n",
    "            plt.hist(class_probs[true_class_mask], bins=20, alpha=0.7, \n",
    "                    label=f'True {class_names[i]}', color='green', density=True)\n",
    "            plt.hist(class_probs[other_class_mask], bins=20, alpha=0.7, \n",
    "                    label='Other classes', color='red', density=True)\n",
    "            \n",
    "            plt.title(f'{class_names[i]} Probability Distribution', fontsize=10, fontweight='bold')\n",
    "            plt.xlabel('Predicted Probability')\n",
    "            plt.ylabel('Density')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 13-16. Error analysis for remaining classes\n",
    "    for i in range(4, min(8, len(class_names))):\n",
    "        if i < len(class_names):\n",
    "            subplot_idx = 9 + (i - 4)\n",
    "            if subplot_idx <= 16:\n",
    "                plt.subplot(4, 4, subplot_idx)\n",
    "                class_probs = probabilities[:, i]\n",
    "                true_class_mask = true_labels == i\n",
    "                other_class_mask = ~true_class_mask\n",
    "                \n",
    "                plt.hist(class_probs[true_class_mask], bins=20, alpha=0.7, \n",
    "                        label=f'True {class_names[i]}', color='green', density=True)\n",
    "                plt.hist(class_probs[other_class_mask], bins=20, alpha=0.7, \n",
    "                        label='Other classes', color='red', density=True)\n",
    "                \n",
    "                plt.title(f'{class_names[i]} Probability Distribution', fontsize=10, fontweight='bold')\n",
    "                plt.xlabel('Predicted Probability')\n",
    "                plt.ylabel('Density')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_evaluation_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create dashboard\n",
    "# create_evaluation_dashboard(true_labels, predictions, probabilities, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90486663",
   "metadata": {},
   "source": [
    "## 4. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors_detailed(true_labels, predictions, probabilities, class_names, top_n=15):\n",
    "    \"\"\"Detailed error analysis\"\"\"\n",
    "    \n",
    "    # Find all errors\n",
    "    error_indices = np.where(predictions != true_labels)[0]\n",
    "    \n",
    "    if len(error_indices) == 0:\n",
    "        print(\"No errors found! Perfect model!\")\n",
    "        return\n",
    "    \n",
    "    # Get error details\n",
    "    error_details = []\n",
    "    for idx in error_indices:\n",
    "        error_info = {\n",
    "            'index': idx,\n",
    "            'true_label': true_labels[idx],\n",
    "            'predicted_label': predictions[idx],\n",
    "            'true_class': class_names[true_labels[idx]],\n",
    "            'predicted_class': class_names[predictions[idx]],\n",
    "            'confidence': probabilities[idx][predictions[idx]],\n",
    "            'true_class_prob': probabilities[idx][true_labels[idx]]\n",
    "        }\n",
    "        error_details.append(error_info)\n",
    "    \n",
    "    # Sort by confidence (most confident errors first)\n",
    "    error_details.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nERROR ANALYSIS\")\n",
    "    print(f\"Found {len(error_indices)} errors out of {len(true_labels)} samples\")\n",
    "    print(f\"Error rate: {len(error_indices)/len(true_labels)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTop {min(top_n, len(error_details))} most confident errors:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, error in enumerate(error_details[:top_n]):\n",
    "        print(f\"Error {i+1} (Index {error['index']}):\")\n",
    "        print(f\"  True: {error['true_class']} (prob: {error['true_class_prob']:.3f})\")\n",
    "        print(f\"  Predicted: {error['predicted_class']} (confidence: {error['confidence']:.3f})\")\n",
    "        print(f\"  Confidence difference: {error['confidence'] - error['true_class_prob']:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    return error_details\n",
    "\n",
    "# Analyze errors\n",
    "# error_analysis = analyze_errors_detailed(true_labels, predictions, probabilities, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e4d5a",
   "metadata": {},
   "source": [
    "## 5. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee119dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_summary(true_labels, predictions, probabilities, class_names):\n",
    "    \"\"\"Generate a comprehensive performance summary\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    max_probs = np.max(probabilities, axis=1)\n",
    "    avg_confidence = np.mean(max_probs)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Overall Accuracy',\n",
    "            'Weighted Precision',\n",
    "            'Weighted Recall',\n",
    "            'Weighted F1-Score',\n",
    "            'Average Confidence',\n",
    "            'Total Test Samples',\n",
    "            'Number of Classes',\n",
    "            'Error Rate'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{accuracy*100:.2f}%\",\n",
    "            f\"{precision:.3f}\",\n",
    "            f\"{recall:.3f}\",\n",
    "            f\"{f1:.3f}\",\n",
    "            f\"{avg_confidence*100:.2f}%\",\n",
    "            f\"{len(true_labels)}\",\n",
    "            f\"{len(class_names)}\",\n",
    "            f\"{(1-accuracy)*100:.2f}%\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Per-class summary\n",
    "    report = classification_report(true_labels, predictions, target_names=class_names, output_dict=True)\n",
    "    class_summary = []\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            class_summary.append({\n",
    "                'Class': class_name,\n",
    "                'Precision': f\"{report[class_name]['precision']:.3f}\",\n",
    "                'Recall': f\"{report[class_name]['recall']:.3f}\",\n",
    "                'F1-Score': f\"{report[class_name]['f1-score']:.3f}\",\n",
    "                'Support': f\"{int(report[class_name]['support'])}\"\n",
    "            })\n",
    "    \n",
    "    class_df = pd.DataFrame(class_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PER-CLASS PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(class_df.to_string(index=False))\n",
    "    \n",
    "    return summary_df, class_df\n",
    "\n",
    "# Generate summary\n",
    "# overall_summary, class_summary = generate_performance_summary(true_labels, predictions, probabilities, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507d9ed",
   "metadata": {},
   "source": [
    "## 6. Save Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results(true_labels, predictions, probabilities, class_names, model_name=\"mridangam_model\"):\n",
    "    \"\"\"Save all evaluation results to files\"\"\"\n",
    "    \n",
    "    # Save detailed classification report\n",
    "    report = classification_report(true_labels, predictions, target_names=class_names, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).T\n",
    "    report_df.to_csv(f'{model_name}_classification_report.csv')\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    cm_df.to_csv(f'{model_name}_confusion_matrix.csv')\n",
    "    \n",
    "    # Save predictions and probabilities\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_label': true_labels,\n",
    "        'predicted_label': predictions,\n",
    "        'true_class': [class_names[i] for i in true_labels],\n",
    "        'predicted_class': [class_names[i] for i in predictions],\n",
    "        'max_probability': np.max(probabilities, axis=1),\n",
    "        'correct': true_labels == predictions\n",
    "    })\n",
    "    \n",
    "    # Add individual class probabilities\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        results_df[f'prob_{class_name}'] = probabilities[:, i]\n",
    "    \n",
    "    results_df.to_csv(f'{model_name}_detailed_results.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nEvaluation results saved:\")\n",
    "    print(f\"- {model_name}_classification_report.csv\")\n",
    "    print(f\"- {model_name}_confusion_matrix.csv\")\n",
    "    print(f\"- {model_name}_detailed_results.csv\")\n",
    "    print(f\"- model_evaluation_dashboard.png\")\n",
    "\n",
    "# Save results\n",
    "# save_evaluation_results(true_labels, predictions, probabilities, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ab632",
   "metadata": {},
   "source": [
    "## 7. Complete Evaluation Pipeline\n",
    "\n",
    "Run this cell to execute the complete evaluation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606bc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_evaluation(model, test_loader, label_encoder, device):\n",
    "    \"\"\"Run the complete evaluation pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting comprehensive model evaluation...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions, true_labels, probabilities = get_predictions_and_labels(model, test_loader, device)\n",
    "    class_names = label_encoder.classes_\n",
    "    \n",
    "    # Display metrics\n",
    "    display_comprehensive_metrics(true_labels, predictions, probabilities, class_names)\n",
    "    \n",
    "    # Create visualization dashboard\n",
    "    create_evaluation_dashboard(true_labels, predictions, probabilities, class_names)\n",
    "    \n",
    "    # Analyze errors\n",
    "    error_analysis = analyze_errors_detailed(true_labels, predictions, probabilities, class_names)\n",
    "    \n",
    "    # Generate summary\n",
    "    overall_summary, class_summary = generate_performance_summary(true_labels, predictions, probabilities, class_names)\n",
    "    \n",
    "    # Save results\n",
    "    save_evaluation_results(true_labels, predictions, probabilities, class_names)\n",
    "    \n",
    "    print(\"\\nEvaluation complete! Check the generated files and plots.\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'probabilities': probabilities,\n",
    "        'overall_summary': overall_summary,\n",
    "        'class_summary': class_summary,\n",
    "        'error_analysis': error_analysis\n",
    "    }\n",
    "\n",
    "# Run complete evaluation\n",
    "# evaluation_results = run_complete_evaluation(model, test_loader, label_encoder, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20db1c",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Load your trained model**: Make sure you have your trained model loaded\n",
    "2. **Prepare test data**: Ensure your test_loader is ready\n",
    "3. **Run evaluation**: Execute the `run_complete_evaluation` function\n",
    "4. **Review results**: Check the generated plots and CSV files\n",
    "\n",
    "### Files Generated:\n",
    "- `model_evaluation_dashboard.png`: Comprehensive visualization dashboard\n",
    "- `{model_name}_classification_report.csv`: Detailed classification metrics\n",
    "- `{model_name}_confusion_matrix.csv`: Confusion matrix data\n",
    "- `{model_name}_detailed_results.csv`: Per-sample predictions and probabilities\n",
    "\n",
    "### Key Metrics to Watch:\n",
    "- **Overall Accuracy**: How often the model is correct\n",
    "- **Per-class F1-scores**: Balanced performance measure for each class\n",
    "- **Confidence distributions**: How certain the model is about its predictions\n",
    "- **Common misclassifications**: Which classes are often confused\n",
    "- **Low-confidence predictions**: Samples the model is uncertain about"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
