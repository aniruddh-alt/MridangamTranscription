{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4fc565",
   "metadata": {},
   "source": [
    "# Simple Real-Time Mridangam Inference\n",
    "\n",
    "A streamlined real-time inference pipeline for mridangam stroke detection.\n",
    "\n",
    "## Features:\n",
    "- Real-time audio capture\n",
    "- Simple onset detection\n",
    "- CNN model inference using modelOutput.pth\n",
    "- Streaming results with yield\n",
    "- Console output of detected strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b99cacdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "⚠️  CUDA not available. Will use CPU.\n",
      "To enable CUDA, install PyTorch with CUDA support:\n",
      "\n",
      "For CUDA 11.8:\n",
      "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
      "\n",
      "For CUDA 12.1:\n",
      "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
      "\n",
      "Then restart the kernel and run this cell again.\n"
     ]
    }
   ],
   "source": [
    "# CUDA Setup and Package Installation\n",
    "\n",
    "# First, check if CUDA is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA not available. Will use CPU.\")\n",
    "    print(\"To enable CUDA, install PyTorch with CUDA support:\")\n",
    "    print(\"\")\n",
    "    print(\"For CUDA 11.8:\")\n",
    "    print(\"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print(\"\")\n",
    "    print(\"For CUDA 12.1:\")\n",
    "    print(\"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "    print(\"\")\n",
    "    print(\"Then restart the kernel and run this cell again.\")\n",
    "\n",
    "# Install other required packages if needed\n",
    "# Uncomment the lines below if you need to install packages:\n",
    "# !pip install librosa pyaudio numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52df5bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "🔧 PyTorch version: 2.7.1+cpu\n",
      "🎵 Librosa version: 0.11.0\n",
      "🎤 CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🎵 Librosa version: {librosa.__version__}\")\n",
    "print(f\"🎤 CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbedf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Architecture (same as training)\n",
    "class MridangamCNN(nn.Module):\n",
    "    def __init__(self, n_mels = 128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            # input shape: (1, 128, 128)\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=1, stride=1),\n",
    "            # output shape: (32, 128, 128)\n",
    "\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # output shape: (32, 64, 64)\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, stride=1),\n",
    "            # output shape: (64, 64, 64)\n",
    "\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # output shape: (64, 32, 32)\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1, stride=1),\n",
    "            # output shape: (64, 32, 32)\n",
    "\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0),\n",
    "            nn.Dropout(p=0.4),\n",
    "            # output shape: (64, 16, 16)\n",
    "            nn.AdaptiveAvgPool2d((1, None)),  # Then reduce spatial\n",
    "\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),  # output shape: (64, 1, 16)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool1d(1),  # output shape: (64, 1, 1)\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Flatten(),  # output shape: (64)\n",
    "            nn.Linear(64, num_classes),  # output shape: (num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2988ac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully from C:\\Users\\aniru\\Desktop\\MridangamTranscription\\inference\\model_weights\\mridangam_model_final.pth\n",
      "🔧 Using device: cpu\n",
      "📊 Classes: bheem, cha, dheem, dhin, num, ta, tha, tham, thi, thom\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "def load_model(model_path: str, device: torch.device) -> MridangamCNN:\n",
    "    \"\"\"Load the trained model from modelOutput.pth\"\"\"\n",
    "    try:\n",
    "        # Create model instance\n",
    "        model = MridangamCNN(n_mels=128, num_classes=10)\n",
    "        \n",
    "        # Load weights\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"✅ Model loaded successfully from {model_path}\")\n",
    "        print(f\"🔧 Using device: {device}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Setup device and load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = r\"C:\\Users\\aniru\\Desktop\\MridangamTranscription\\inference\\model_weights\\mridangam_model_final.pth\"  # Adjust path as needed\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_path, device)\n",
    "\n",
    "# Default class names (update these based on your training data)\n",
    "class_names = ['bheem','cha', 'dheem', 'dhin','num', 'ta', 'tha', 'tham', 'thi', 'thom']\n",
    "print(f\"📊 Classes: {', '.join(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db9132e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced audio processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Audio processing functions\n",
    "def detect_onset(audio_buffer: np.ndarray, sr: float) -> Optional[float]:\n",
    "    \"\"\"Enhanced onset detection for clear mridangam strokes only\"\"\"\n",
    "    if len(audio_buffer) < sr * 0.15:  # Need at least 150ms of audio\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Use spectral flux for onset detection with stricter parameters\n",
    "        onset_envelope = librosa.onset.onset_strength(\n",
    "            y=audio_buffer, \n",
    "            sr=sr,\n",
    "            aggregate=np.median,  # Use median for more stable detection\n",
    "            fmax=8000,  # Focus on relevant frequency range for mridangam\n",
    "            n_mels=128\n",
    "        )\n",
    "        \n",
    "        # Detect onsets with stricter thresholds\n",
    "        onsets = librosa.onset.onset_detect(\n",
    "            onset_envelope=onset_envelope,\n",
    "            sr=sr, \n",
    "            units='time',\n",
    "            pre_max=5,   # Increased for clearer peaks\n",
    "            post_max=5, \n",
    "            pre_avg=5, \n",
    "            post_avg=5,\n",
    "            delta=0.3,   # Higher threshold for clearer onsets\n",
    "            wait=10      # Longer wait between onsets\n",
    "        )\n",
    "        \n",
    "        if len(onsets) > 0:\n",
    "            # Get the most recent onset\n",
    "            latest_onset = onsets[-1]\n",
    "            \n",
    "            # Validate onset strength - only return if it's a strong onset\n",
    "            onset_idx = int(latest_onset * sr / 512)  # Convert to frame index\n",
    "            if onset_idx < len(onset_envelope):\n",
    "                onset_strength = onset_envelope[onset_idx]\n",
    "                \n",
    "                # Only return onset if it's significantly stronger than average\n",
    "                avg_strength = np.mean(onset_envelope)\n",
    "                if onset_strength > avg_strength * 2.0:  # Must be 2x stronger than average\n",
    "                    return latest_onset\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Onset detection error: {e}\")\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_mel_spectrogram(audio: np.ndarray, sr: float, n_mels=128, target_length=128) -> np.ndarray:\n",
    "    \"\"\"Extract mel spectrogram from audio segment\"\"\"\n",
    "    try:\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=sr,\n",
    "            n_mels=n_mels,\n",
    "            hop_length=512,\n",
    "            win_length=1024,\n",
    "            n_fft=2048\n",
    "        )\n",
    "        \n",
    "        # Convert to dB\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Pad or truncate to target length\n",
    "        if mel_spec_db.shape[1] < target_length:\n",
    "            pad_width = target_length - mel_spec_db.shape[1]\n",
    "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mel_spec_db = mel_spec_db[:, :target_length]\n",
    "            \n",
    "        return mel_spec_db\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Mel spectrogram extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_audio_segment(audio_segment: np.ndarray, sr: float) -> bool:\n",
    "    \"\"\"Validate that audio segment contains sufficient energy for prediction\"\"\"\n",
    "    if len(audio_segment) < sr * 0.05:  # At least 50ms\n",
    "        return False\n",
    "        \n",
    "    # Check RMS energy\n",
    "    rms = np.sqrt(np.mean(audio_segment**2))\n",
    "    if rms < 0.01:  # Too quiet\n",
    "        return False\n",
    "        \n",
    "    # Check for dynamic range\n",
    "    peak = np.max(np.abs(audio_segment))\n",
    "    if peak < 0.05:  # Too low amplitude\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "def predict_stroke(audio_segment: np.ndarray, sample_rate: int) -> Tuple[str, float]:\n",
    "    \"\"\"Predict stroke from audio segment with validation\"\"\"\n",
    "    try:\n",
    "        # First validate the audio segment\n",
    "        if not validate_audio_segment(audio_segment, sample_rate):\n",
    "            return \"Weak_Signal\", 0.0\n",
    "        \n",
    "        # Extract mel spectrogram\n",
    "        mel_spec = extract_mel_spectrogram(audio_segment, sample_rate)\n",
    "        if mel_spec is None:\n",
    "            return \"Unknown\", 0.0\n",
    "        \n",
    "        # Prepare input tensor\n",
    "        input_tensor = torch.FloatTensor(mel_spec).unsqueeze(0).unsqueeze(0)  # (1, 1, n_mels, time)\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        # Model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        predicted_stroke = class_names[predicted_class]\n",
    "        return predicted_stroke, confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "print(\"✅ Enhanced audio processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a35f2171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio listener class defined!\n"
     ]
    }
   ],
   "source": [
    "# Simple audio listener class\n",
    "class SimpleAudioListener:\n",
    "    \"\"\"Simple real-time audio capture and processing\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050, chunk_size=1024, channels=1):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_size = chunk_size\n",
    "        self.channels = channels\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.is_recording = False\n",
    "        \n",
    "        # Initialize PyAudio\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        \n",
    "    def start_recording(self):\n",
    "        \"\"\"Start audio recording in a separate thread\"\"\"\n",
    "        self.is_recording = True\n",
    "        \n",
    "        stream = self.p.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=self.channels,\n",
    "            rate=self.sample_rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.chunk_size\n",
    "        )\n",
    "        \n",
    "        def record_audio():\n",
    "            while self.is_recording:\n",
    "                try:\n",
    "                    data = stream.read(self.chunk_size, exception_on_overflow=False)\n",
    "                    audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "                    self.audio_queue.put(audio_chunk)\n",
    "                except Exception as e:\n",
    "                    print(f\"Audio recording error: {e}\")\n",
    "                    break\n",
    "        \n",
    "        self.recording_thread = threading.Thread(target=record_audio)\n",
    "        self.recording_thread.daemon = True\n",
    "        self.recording_thread.start()\n",
    "        \n",
    "        print(f\"🎤 Started recording at {self.sample_rate}Hz...\")\n",
    "        \n",
    "    def stop_recording(self):\n",
    "        \"\"\"Stop audio recording\"\"\"\n",
    "        self.is_recording = False\n",
    "        if hasattr(self, 'recording_thread'):\n",
    "            self.recording_thread.join()\n",
    "        self.p.terminate()\n",
    "        print(\"🛑 Stopped recording\")\n",
    "        \n",
    "    def get_audio_chunk(self, timeout=0.1):\n",
    "        \"\"\"Get audio chunk from queue\"\"\"\n",
    "        try:\n",
    "            return self.audio_queue.get(timeout=timeout)\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "\n",
    "print(\"✅ Audio listener class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e30faadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced real-time detection function ready!\n"
     ]
    }
   ],
   "source": [
    "# Main real-time detection function\n",
    "def run_realtime_detection(duration_seconds=30):\n",
    "    \"\"\"Run real-time stroke detection with clear onset validation\"\"\"\n",
    "    \n",
    "    # Audio settings\n",
    "    sample_rate = 22050\n",
    "    buffer_duration = 3.0  # Increased buffer for better onset detection\n",
    "    buffer_size = int(sample_rate * buffer_duration)\n",
    "    audio_buffer = np.zeros(buffer_size)\n",
    "    \n",
    "    # Initialize audio listener\n",
    "    audio_listener = SimpleAudioListener(sample_rate=sample_rate)\n",
    "    \n",
    "    # Detection parameters - stricter settings\n",
    "    last_detection_time = 0\n",
    "    detection_cooldown = 0.3  # Longer cooldown to avoid rapid false detections\n",
    "    confidence_threshold = 0.6  # Higher confidence threshold\n",
    "    \n",
    "    print(f\"🎤 Starting real-time detection for {duration_seconds} seconds...\")\n",
    "    print(\"🎵 Listening for CLEAR mridangam strokes only...\")\n",
    "    print(\"💡 Tip: Play mridangam strokes clearly and distinctly\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    audio_listener.start_recording()\n",
    "    start_time = time.time()\n",
    "    detection_count = 0\n",
    "    \n",
    "    try:\n",
    "        while (time.time() - start_time) < duration_seconds:\n",
    "            # Get new audio chunk\n",
    "            chunk = audio_listener.get_audio_chunk(timeout=0.05)\n",
    "            if chunk is not None:\n",
    "                # Update circular buffer\n",
    "                audio_buffer = np.roll(audio_buffer, -len(chunk))\n",
    "                audio_buffer[-len(chunk):] = chunk\n",
    "            \n",
    "            # Check for onset in recent audio\n",
    "            current_time = time.time()\n",
    "            if current_time - last_detection_time > detection_cooldown:\n",
    "                \n",
    "                # Look for onset in recent audio (last 1 second for better analysis)\n",
    "                recent_audio = audio_buffer[-int(1.0 * sample_rate):]\n",
    "                onset_time = detect_onset(recent_audio, sample_rate)\n",
    "                \n",
    "                if onset_time is not None:\n",
    "                    # Extract segment around onset with better timing\n",
    "                    pre_onset = int(0.05 * sample_rate)   # 50ms before\n",
    "                    post_onset = int(0.2 * sample_rate)   # 200ms after (increased)\n",
    "                    \n",
    "                    onset_sample = int(onset_time * sample_rate)\n",
    "                    start_idx = max(0, len(recent_audio) - onset_sample - pre_onset)\n",
    "                    end_idx = min(len(recent_audio), len(recent_audio) - onset_sample + post_onset)\n",
    "                    \n",
    "                    stroke_segment = recent_audio[start_idx:end_idx]\n",
    "                    \n",
    "                    if len(stroke_segment) > 0:\n",
    "                        # Predict stroke\n",
    "                        stroke_name, confidence = predict_stroke(stroke_segment, sample_rate)\n",
    "                        \n",
    "                        # Only report high-confidence predictions from clear onsets\n",
    "                        if confidence > confidence_threshold and stroke_name not in ['Weak_Signal', 'Unknown', 'Error']:\n",
    "                            timestamp = current_time\n",
    "                            last_detection_time = current_time\n",
    "                            detection_count += 1\n",
    "                            \n",
    "                            # Format output\n",
    "                            time_str = time.strftime(\"%H:%M:%S\", time.localtime(timestamp))\n",
    "                            elapsed = timestamp - start_time\n",
    "                            \n",
    "                            # Color coding based on confidence\n",
    "                            confidence_bar = \"🟢\" if confidence > 0.8 else \"🟡\" if confidence > 0.7 else \"🔴\"\n",
    "                            \n",
    "                            print(f\"{time_str} | +{elapsed:5.1f}s | {confidence_bar} {stroke_name:>8} | {confidence:.1%} | #{detection_count}\")\n",
    "                        elif stroke_name == 'Weak_Signal':\n",
    "                            # Optionally show weak signals for debugging\n",
    "                            # print(f\"  💨 Weak signal detected (not classified)\")\n",
    "                            pass\n",
    "            \n",
    "            time.sleep(0.01)  # Small delay to prevent busy waiting\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Detection stopped by user\")\n",
    "    finally:\n",
    "        audio_listener.stop_recording()\n",
    "        print(f\"✅ Detection completed - Found {detection_count} clear strokes\")\n",
    "\n",
    "print(\"✅ Enhanced real-time detection function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fc0f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 Starting real-time detection for 30 seconds...\n",
      "🎵 Listening for CLEAR mridangam strokes only...\n",
      "💡 Tip: Play mridangam strokes clearly and distinctly\n",
      "============================================================\n",
      "🎤 Started recording at 22050Hz...\n",
      "11:13:47 | +  2.6s | 🟢       ta | 99.4% | #1\n",
      "11:13:48 | +  3.8s | 🟢    dheem | 100.0% | #2\n",
      "11:13:49 | +  5.1s | 🟢    dheem | 100.0% | #3\n",
      "11:13:51 | +  6.4s | 🟢    dheem | 100.0% | #4\n",
      "11:13:52 | +  7.7s | 🟢       ta | 98.1% | #5\n",
      "11:13:53 | +  9.0s | 🟢    dheem | 100.0% | #6\n",
      "11:13:55 | + 10.3s | 🟢    dheem | 98.7% | #7\n",
      "11:13:56 | + 11.9s | 🟢       ta | 98.7% | #8\n",
      "11:13:56 | + 12.2s | 🟢       ta | 93.5% | #9\n",
      "11:13:57 | + 12.9s | 🟢      tha | 95.9% | #10\n",
      "11:13:58 | + 13.3s | 🟢    dheem | 100.0% | #11\n",
      "11:13:58 | + 13.6s | 🟢       ta | 99.7% | #12\n",
      "11:13:58 | + 14.3s | 🟡     thom | 73.5% | #13\n",
      "11:13:59 | + 14.6s | 🔴       ta | 62.6% | #14\n",
      "11:13:59 | + 15.0s | 🟢       ta | 93.2% | #15\n",
      "11:14:00 | + 16.0s | 🟢    dheem | 81.4% | #16\n",
      "11:14:01 | + 16.9s | 🟢    dheem | 100.0% | #17\n",
      "11:14:01 | + 17.2s | 🟢     thom | 100.0% | #18\n",
      "11:14:02 | + 17.6s | 🟢     thom | 100.0% | #19\n",
      "11:14:02 | + 17.9s | 🔴     thom | 61.2% | #20\n",
      "11:14:02 | + 18.2s | 🟢       ta | 99.7% | #21\n",
      "11:14:03 | + 18.5s | 🟢      tha | 99.6% | #22\n",
      "11:14:03 | + 18.9s | 🟢     thom | 100.0% | #23\n",
      "11:14:03 | + 19.2s | 🟢      tha | 98.0% | #24\n",
      "11:14:04 | + 19.5s | 🟢     thom | 99.5% | #25\n",
      "11:14:04 | + 19.9s | 🟢       ta | 96.7% | #26\n",
      "11:14:04 | + 20.2s | 🟢     thom | 99.5% | #27\n",
      "11:14:05 | + 20.8s | 🟢    dheem | 100.0% | #28\n",
      "11:14:05 | + 21.1s | 🟢     thom | 99.9% | #29\n",
      "11:14:06 | + 21.5s | 🟢     thom | 99.9% | #30\n",
      "11:14:06 | + 21.8s | 🟢       ta | 81.2% | #31\n",
      "11:14:06 | + 22.1s | 🟢       ta | 95.8% | #32\n",
      "11:14:07 | + 22.4s | 🟢     thom | 100.0% | #33\n",
      "11:14:07 | + 22.8s | 🟢       ta | 99.7% | #34\n",
      "11:14:07 | + 23.1s | 🟢     thom | 99.2% | #35\n",
      "11:14:08 | + 23.4s | 🟢       ta | 99.9% | #36\n",
      "11:14:08 | + 23.7s | 🟢     thom | 100.0% | #37\n",
      "11:14:08 | + 24.1s | 🟢     thom | 99.3% | #38\n",
      "11:14:09 | + 24.4s | 🟡     thom | 79.4% | #39\n",
      "11:14:09 | + 24.7s | 🔴     thom | 60.6% | #40\n",
      "11:14:09 | + 25.0s | 🟢     thom | 100.0% | #41\n",
      "11:14:10 | + 25.4s | 🟢       ta | 81.4% | #42\n",
      "11:14:10 | + 25.7s | 🟢     thom | 99.6% | #43\n",
      "11:14:10 | + 26.0s | 🟢     thom | 100.0% | #44\n",
      "11:14:11 | + 26.3s | 🟢     thom | 80.5% | #45\n",
      "11:14:11 | + 26.7s | 🔴       ta | 60.5% | #46\n",
      "11:14:11 | + 27.0s | 🟡       ta | 71.4% | #47\n",
      "11:14:12 | + 27.3s | 🟢     thom | 97.5% | #48\n",
      "🛑 Stopped recording\n",
      "✅ Detection completed - Found 48 clear strokes\n"
     ]
    }
   ],
   "source": [
    "# Run the real-time detection\n",
    "# Adjust duration as needed (in seconds)\n",
    "run_realtime_detection(duration_seconds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d265a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Enhanced Audio File Inference\n",
    "def analyze_audio_file(audio_file_path, show_details=True, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Analyze an audio file for mridangam strokes with enhanced onset detection.\n",
    "    Only predicts when clear onsets are detected.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(audio_file_path, sr=22050)\n",
    "        print(f\"📁 Loaded: {audio_file_path}\")\n",
    "        print(f\"🎵 Duration: {len(audio)/sr:.2f} seconds\")\n",
    "        print(f\"🔊 Sample rate: {sr} Hz\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Enhanced onset detection (same as real-time)\n",
    "        onset_envelope = librosa.onset.onset_strength(\n",
    "            y=audio, \n",
    "            sr=sr,\n",
    "            aggregate=np.median,  # Use median for more stable detection\n",
    "            fmax=8000,  # Focus on relevant frequency range for mridangam\n",
    "            n_mels=128\n",
    "        )\n",
    "        \n",
    "        # Detect onsets with stricter thresholds\n",
    "        onsets = librosa.onset.onset_detect(\n",
    "            onset_envelope=onset_envelope,\n",
    "            sr=sr, \n",
    "            units='time',\n",
    "            pre_max=5,   # Increased for clearer peaks\n",
    "            post_max=5, \n",
    "            pre_avg=5, \n",
    "            post_avg=5,\n",
    "            delta=0.3,   # Higher threshold for clearer onsets\n",
    "            wait=10      # Longer wait between onsets\n",
    "        )\n",
    "        \n",
    "        print(f\"🎯 Found {len(onsets)} potential onsets\")\n",
    "        \n",
    "        valid_predictions = []\n",
    "        \n",
    "        # Process each onset with validation\n",
    "        for i, onset_time in enumerate(onsets):\n",
    "            # Validate onset strength\n",
    "            onset_idx = int(onset_time * sr / 512)  # Convert to frame index\n",
    "            if onset_idx < len(onset_envelope):\n",
    "                onset_strength = onset_envelope[onset_idx]\n",
    "                avg_strength = np.mean(onset_envelope)\n",
    "                \n",
    "                # Only process if it's a strong onset\n",
    "                if onset_strength > avg_strength * 2.0:\n",
    "                    # Extract segment around onset\n",
    "                    pre_onset = int(0.05 * sr)   # 50ms before\n",
    "                    post_onset = int(0.2 * sr)   # 200ms after\n",
    "                    \n",
    "                    onset_sample = int(onset_time * sr)\n",
    "                    start_idx = max(0, onset_sample - pre_onset)\n",
    "                    end_idx = min(len(audio), onset_sample + post_onset)\n",
    "                    \n",
    "                    segment = audio[start_idx:end_idx]\n",
    "                    \n",
    "                    if len(segment) > 0:\n",
    "                        stroke_name, confidence = predict_stroke(segment, sr)\n",
    "                        \n",
    "                        # Only include high-confidence predictions from validated onsets\n",
    "                        if (confidence > confidence_threshold and \n",
    "                            stroke_name not in ['Weak_Signal', 'Unknown', 'Error']):\n",
    "                            \n",
    "                            valid_predictions.append({\n",
    "                                'time': onset_time,\n",
    "                                'stroke': stroke_name,\n",
    "                                'confidence': confidence,\n",
    "                                'onset_strength': onset_strength / avg_strength\n",
    "                            })\n",
    "                            \n",
    "                            if show_details:\n",
    "                                # Color coding based on confidence\n",
    "                                confidence_bar = \"🟢\" if confidence > 0.8 else \"🟡\" if confidence > 0.7 else \"🔴\"\n",
    "                                strength_indicator = \"⚡\" if onset_strength > avg_strength * 3.0 else \"🔥\"\n",
    "                                \n",
    "                                print(f\"  {i+1:2d}. {onset_time:6.2f}s | {confidence_bar} {stroke_name:>8} | {confidence:.1%} | {strength_indicator} {onset_strength/avg_strength:.1f}x\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"📊 ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total onsets detected: {len(onsets)}\")\n",
    "        print(f\"Valid stroke predictions: {len(valid_predictions)}\")\n",
    "        print(f\"Confidence threshold: {confidence_threshold:.1%}\")\n",
    "        \n",
    "        if valid_predictions:\n",
    "            # Count strokes by type\n",
    "            stroke_counts = {}\n",
    "            total_confidence = 0\n",
    "            \n",
    "            for pred in valid_predictions:\n",
    "                stroke = pred['stroke']\n",
    "                if stroke in stroke_counts:\n",
    "                    stroke_counts[stroke] += 1\n",
    "                else:\n",
    "                    stroke_counts[stroke] = 1\n",
    "                total_confidence += pred['confidence']\n",
    "            \n",
    "            avg_confidence = total_confidence / len(valid_predictions)\n",
    "            \n",
    "            print(f\"Average confidence: {avg_confidence:.1%}\")\n",
    "            print(\"\\nDetected strokes:\")\n",
    "            for stroke, count in sorted(stroke_counts.items()):\n",
    "                print(f\"  {stroke}: {count} times\")\n",
    "            \n",
    "            # Timeline of strokes\n",
    "            print(\"\\nStroke timeline:\")\n",
    "            timeline = ' → '.join([f\"{p['stroke']}({p['time']:.1f}s)\" for p in valid_predictions[:10]])\n",
    "            if len(valid_predictions) > 10:\n",
    "                timeline += f\" ... (+{len(valid_predictions)-10} more)\"\n",
    "            print(f\"  {timeline}\")\n",
    "        else:\n",
    "            print(\"❌ No clear mridangam strokes detected above threshold\")\n",
    "            print(\"💡 Try:\")\n",
    "            print(\"   - Lowering confidence_threshold (e.g., 0.4)\")\n",
    "            print(\"   - Using audio with clearer mridangam strokes\")\n",
    "            print(\"   - Checking if the audio contains mridangam sounds\")\n",
    "        \n",
    "        return valid_predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing file: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ Enhanced audio file analysis function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze your audio file\n",
    "# Replace the path below with your audio file path\n",
    "\n",
    "# Example usage:\n",
    "audio_file_path = r\"C:\\path\\to\\your\\audio\\file.wav\"  # Change this path\n",
    "\n",
    "# Uncomment and modify the path to analyze your audio file:\n",
    "# results = analyze_audio_file(audio_file_path, show_details=True, confidence_threshold=0.6)\n",
    "\n",
    "# You can also adjust the confidence threshold:\n",
    "# results = analyze_audio_file(audio_file_path, show_details=True, confidence_threshold=0.4)  # Lower threshold\n",
    "# results = analyze_audio_file(audio_file_path, show_details=True, confidence_threshold=0.8)  # Higher threshold\n",
    "\n",
    "print(\"💡 Instructions:\")\n",
    "print(\"1. Update the 'audio_file_path' variable with your audio file path\")\n",
    "print(\"2. Uncomment one of the 'analyze_audio_file' calls\")\n",
    "print(\"3. Run this cell to analyze your audio file\")\n",
    "print(\"\")\n",
    "print(\"Supported formats: .wav, .mp3, .flac, .m4a, .ogg\")\n",
    "print(\"Recommended: .wav files with clear mridangam recordings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98ff0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
